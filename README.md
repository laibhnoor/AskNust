# AI-Powered RAG Chatbot (FastAPI + LangChain) #
 
This project is a Retrieval-Augmented Generation (RAG) chatbot built with FastAPI and LangChain that allows users to query documents (PDFs, CSVs, etc.) and get context-aware answers in natural language.

It uses FAISS for vector storage, OpenAI GPT (or HuggingFace models) for LLM responses, and supports conversational memory so that follow-up questions remain contextual.

üöÄ Features

‚úÖ Document Loader ‚Äì Loads multiple PDFs and CSVs from a data/ folder

‚úÖ Chunking & Embeddings ‚Äì Splits documents into manageable chunks and generates vector embeddings

‚úÖ FAISS Vector Database ‚Äì Stores document embeddings locally for fast retrieval

‚úÖ RAG Pipeline ‚Äì Retrieves most relevant chunks and passes them to an LLM for answer generation

‚úÖ Conversational Memory ‚Äì Remembers chat history for contextual follow-ups

‚úÖ FastAPI Backend ‚Äì Provides REST API endpoints for querying

‚úÖ CORS Enabled ‚Äì Works seamlessly with React (or any frontend)


##  üõ†Ô∏è Tech Stack ##

Backend: FastAPI

Document Processing: LangChain + FAISS

LLM: OpenAI GPT-3.5 (configurable)

Embeddings: OpenAI Embeddings or HuggingFace Sentence Transformers

Memory: ConversationBufferMemory (LangChain)

## ‚ö° Setup & Usage ##    
### 1Ô∏è‚É£ Clone the Repository
git clone https://github.com/laibhnoor/rag-chatbot.git
cd rag-chatbot/backend

### 2Ô∏è‚É£ Install Dependencies
pip install -r requirements.txt

### 3Ô∏è‚É£ Configure Environment Variables
Create a .env file in the backend/ folder:
OPENAI_API_KEY=your_openai_api_key_here

### 4Ô∏è‚É£ Build the Vector Database
python ingest.py

This will:

Load all PDFs/CSVs from data/

Split into chunks

Create embeddings

Save FAISS index locally in vectorstore/

### 5Ô∏è‚É£ Run the FastAPI Server
uvicorn main:app --reload

You should see:

‚úÖ FastAPI is running

Visit http://localhost:8000 to confirm.






